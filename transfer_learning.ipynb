{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import pickle\n",
    "from torch.optim import Adam\n",
    "import argparse\n",
    "import copy\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Get mappings </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "on2onlabel = dict()\n",
    "for idx, name in enumerate(os.listdir('/storage/jalverio/objectnet-oct-24-d123')):\n",
    "    on2onlabel[name] = idx\n",
    "onlabel2name = {v: k for k, v in on2onlabel.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/storage/jalverio/resnet/objectnet2torch.pkl', 'rb') as f:\n",
    "    objectnet2torch = pickle.load(f)\n",
    "torch2objectnet = dict()\n",
    "for objectnet_name, label_list in objectnet2torch.items():\n",
    "    for label in label_list:\n",
    "        torch2objectnet[label] = objectnet_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/storage/jalverio/resnet/dirname_to_objectnet_name.json') as f:\n",
    "    dirname_to_classname = json.load(f)\n",
    "\n",
    "with open('/storage/jalverio/resnet/objectnet_subset_to_objectnet_id') as f:\n",
    "    oncompressed2onlabel = eval(f.read())\n",
    "    onlabel2oncompressed = {int(v):int(k) for k,v in oncompressed2onlabel.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Build Dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transformations = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 113, 313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Objectnet(Dataset):\n",
    "#     \"\"\"Dataset wrapping images and target labels for Kaggle - Planet Amazon from Space competition.\n",
    "#     Arguments:\n",
    "#         A CSV file path\n",
    "#         Path to image folder\n",
    "#         Extension of images\n",
    "#         PIL transforms\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, root, transform, objectnet2imagenet, imagenet2torch):\n",
    "#         self.root = root\n",
    "#         self.transform = transform\n",
    "#         self.images = []\n",
    "#         success_counter = 0\n",
    "#         for dirname in os.listdir(root):\n",
    "#             class_name = dirname.replace('/', '_').replace('-', '_').replace(' ', '_').lower().replace(\"'\", '')\n",
    "#             if class_name not in objectnet2imagenet:\n",
    "#                 continue\n",
    "#             success_counter += 1\n",
    "#             labels = objectnet2imagenet[class_name]\n",
    "#             new_labels = []\n",
    "#             for label in labels:\n",
    "#                 new_labels.append(int(imagenet2torch[label - 1]))\n",
    "\n",
    "#             images = os.listdir(os.path.join(root, dirname))\n",
    "#             for image_name in images:\n",
    "#                 path = os.path.join(root, dirname, image_name)\n",
    "#                 self.images.append((path, new_labels))\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         full_path, labels = self.images[index]\n",
    "#         image = Image.open(full_path)\n",
    "#         image = image.convert('RGB')\n",
    "#         image = self.transform(image)\n",
    "#         return image, labels\n",
    "\n",
    "#     def n_per_class(self, num_examples, valid_classes):\n",
    "#         quotas = dict()\n",
    "#         for label in valid_classes:\n",
    "#             quotas[label] = num_examples\n",
    "#         remaining_images = []\n",
    "#         for path, label in self.images:\n",
    "#             if label in valid_classes:\n",
    "#                 if quotas[label] < 0:\n",
    "#                     quotas[label] -= 1\n",
    "#                     remaining_images.append((path, label))\n",
    "#         self.images = remaining_images\n",
    "#         print('Purged some examples. %s classes and %s examples remaining.' % (len(valid_classes), len(self.images)))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objectnet(Dataset):\n",
    "    def __init__(self, root, transform, onlabel2oncompressed, num_examples, overlap, test_images=None):\n",
    "        self.transform = transform\n",
    "        if test_images is None:\n",
    "            self.classes_in_dataset = set()\n",
    "            images_dict = dict()\n",
    "            for dirname in os.listdir(root):\n",
    "                label = on2onlabel[dirname]\n",
    "                if overlap:\n",
    "                    if label not in onlabel2oncompressed:\n",
    "                        continue\n",
    "                    label = onlabel2oncompressed[label]\n",
    "                    class_name = dirname_to_classname[dirname]\n",
    "                images = os.listdir(os.path.join(root, dirname))\n",
    "                if len(images) < num_examples:\n",
    "                    continue\n",
    "                for image_name in images:\n",
    "                    path = os.path.join(root, dirname, image_name)\n",
    "                    if label not in images_dict:\n",
    "                        images_dict[label] = []\n",
    "                    images_dict[label].append(path)\n",
    "                self.classes_in_dataset.add(dirname)\n",
    "            self.images = []\n",
    "            self.test_images = []\n",
    "            for label in images_dict.keys():\n",
    "                idxs_to_choose_from = list(range(len(images_dict[label])))\n",
    "                chosen_idxs = np.random.choice(idxs_to_choose_from, num_examples, replace=False)\n",
    "                class_training_idxs = set(chosen_idxs.tolist())\n",
    "                class_training_images = [images_dict[label][idx] for idx in class_training_idxs]\n",
    "                if len(class_training_images) != 8:\n",
    "                    import pdb; pdb.set_trace()\n",
    "                test_training_idxs = [x for x in range(len(images_dict[label])) if x not in class_training_idxs]\n",
    "                class_test_images = [images_dict[label][idx] for idx in test_training_idxs]\n",
    "                [self.images.append((image, label)) for image in class_training_images]\n",
    "                [self.test_images.append((image, label)) for image in class_test_images]\n",
    "            print('Dataset has %s classes, %s training examples and %s test examples' % (len(self.classes_in_dataset), len(self.images), len(self.test_images)))\n",
    "        else:\n",
    "            self.images = test_images\n",
    "\n",
    "#     def n_per_class(self, num_examples, test):\n",
    "#         valid_classes = set()\n",
    "#         [valid_classes.add(label) for _, label in self.images]\n",
    "\n",
    "#         quotas = dict()\n",
    "#         for label in valid_classes:\n",
    "#             quotas[label] = 0\n",
    "#         test_images = []\n",
    "#         remaining_images = []\n",
    "#         for path, objectnet_label in self.images:\n",
    "#             if not test:\n",
    "#                 if quotas[objectnet_label] < num_examples:\n",
    "#                     quotas[objectnet_label] += 1\n",
    "#                     remaining_images.append((path, objectnet_label))\n",
    "#                 else:\n",
    "#                     test_images.append((path, objectnet_label))\n",
    "#             else:\n",
    "#                 if quotas[objectnet_label] < num_examples * 2:\n",
    "#                     if quotas[objectnet_label] >= num_examples:\n",
    "#                         remaining_images.append((path, objectnet_label))\n",
    "#                     quotas[objectnet_label] += 1\n",
    "#                 else:\n",
    "#                     test_images.append((path, objectnet_label))\n",
    "#         self.images = remaining_images\n",
    "#         self.test_images = test_images\n",
    "#         print('Removed some examples. %s classes and %s examples remaining.' % (len(valid_classes), len(self.images)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        full_path, labels = self.images[index]\n",
    "        image = Image.open(full_path)\n",
    "        image = image.convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        return image, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Helper functions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    total_top1, total_top5, total_examples = 0, 0, 0\n",
    "    for batch_counter, (batch, labels) in enumerate(test_loader):\n",
    "        print(batch_counter / len(test_loader))\n",
    "        labels = labels.to(DEVICE)\n",
    "        batch = batch.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch)\n",
    "        top1, top5 = accuracy(logits, labels)\n",
    "        total_top1 += top1\n",
    "        total_top5 += top5\n",
    "        total_examples += batch.shape[0]\n",
    "    top1_score = total_top1 / total_examples\n",
    "    top5_score = total_top5 / total_examples\n",
    "    return top1_score, top5_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, targets):\n",
    "    _, pred = logits.topk(5, 1, True, True)\n",
    "    targets = targets.unsqueeze(1)\n",
    "    targets_repeat = targets.repeat(1, 5)\n",
    "    assert pred.shape == targets_repeat.shape\n",
    "    correct = ((pred - targets_repeat) == 0).float()\n",
    "    top1_score = correct[:, 0].sum()\n",
    "    top5_score = correct.sum()\n",
    "    return top1_score.item(), top5_score.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> params and model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "WORKERS = 60\n",
    "BATCH_SIZE = 32\n",
    "NUM_EXAMPLES = 8\n",
    "OVERLAP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 113 classes, 904 training examples and 16990 test examples\n"
     ]
    }
   ],
   "source": [
    "image_dir = '/storage/jalverio/objectnet-oct-24-d123/'\n",
    "dataset = Objectnet(image_dir, transformations, onlabel2oncompressed, NUM_EXAMPLES, OVERLAP, test_images=None)\n",
    "dataset_test = Objectnet(image_dir, transformations, onlabel2oncompressed, NUM_EXAMPLES, OVERLAP, test_images=dataset.test_images)\n",
    "total_classes = len(dataset.classes_in_dataset)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=512, shuffle=False,\n",
    "        num_workers=WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet152(pretrained=True).eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(2048, total_classes, bias=True)\n",
    "model = model.eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Training <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "previous_accuracy = 0.\n",
    "top_score = 0.\n",
    "total_top1, total_top5, total_examples = 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 0\n",
      "training top1 score: 0.9170353982300885\n",
      "training top5 score: 0.995575221238938\n",
      "starting epoch 1\n",
      "training top1 score: 0.9192477876106194\n",
      "training top5 score: 0.995575221238938\n",
      "starting epoch 2\n",
      "training top1 score: 0.9214601769911505\n",
      "training top5 score: 0.995575221238938\n",
      "starting epoch 3\n",
      "training top1 score: 0.9280973451327433\n",
      "training top5 score: 0.995575221238938\n",
      "starting epoch 4\n",
      "training top1 score: 0.9314159292035398\n",
      "training top5 score: 0.995575221238938\n",
      "starting epoch 5\n",
      "training top1 score: 0.9325221238938053\n",
      "training top5 score: 0.9966814159292036\n",
      "starting epoch 6\n",
      "training top1 score: 0.9369469026548672\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 7\n",
      "training top1 score: 0.9369469026548672\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 8\n",
      "training top1 score: 0.9402654867256637\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 9\n",
      "training top1 score: 0.9435840707964602\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 10\n",
      "training top1 score: 0.9469026548672567\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 11\n",
      "training top1 score: 0.9480088495575221\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 12\n",
      "training top1 score: 0.9480088495575221\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 13\n",
      "training top1 score: 0.9491150442477876\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 14\n",
      "training top1 score: 0.9513274336283186\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 15\n",
      "training top1 score: 0.9535398230088495\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 16\n",
      "training top1 score: 0.9557522123893806\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 17\n",
      "training top1 score: 0.956858407079646\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 18\n",
      "training top1 score: 0.9601769911504425\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 19\n",
      "training top1 score: 0.9646017699115044\n",
      "training top5 score: 0.9977876106194691\n",
      "starting epoch 20\n",
      "training top1 score: 0.9679203539823009\n",
      "training top5 score: 0.9988938053097345\n",
      "starting epoch 21\n",
      "training top1 score: 0.9701327433628318\n",
      "training top5 score: 0.9988938053097345\n",
      "starting epoch 22\n",
      "training top1 score: 0.9712389380530974\n",
      "training top5 score: 0.9988938053097345\n",
      "starting epoch 23\n",
      "training top1 score: 0.9734513274336283\n",
      "training top5 score: 0.9988938053097345\n",
      "starting epoch 24\n",
      "training top1 score: 0.9745575221238938\n",
      "training top5 score: 0.9988938053097345\n",
      "starting epoch 25\n",
      "training top1 score: 0.9745575221238938\n",
      "training top5 score: 0.9988938053097345\n",
      "starting epoch 26\n",
      "training top1 score: 0.9778761061946902\n",
      "training top5 score: 0.9988938053097345\n",
      "starting epoch 27\n",
      "training top1 score: 0.9778761061946902\n",
      "training top5 score: 0.9988938053097345\n",
      "starting epoch 28\n",
      "training top1 score: 0.9800884955752213\n",
      "training top5 score: 0.9988938053097345\n",
      "starting epoch 29\n",
      "training top1 score: 0.9811946902654868\n",
      "training top5 score: 0.9988938053097345\n",
      "starting epoch 30\n",
      "training top1 score: 0.9823008849557522\n",
      "training top5 score: 1.0\n",
      "starting epoch 31\n",
      "training top1 score: 0.9845132743362832\n",
      "training top5 score: 1.0\n",
      "starting epoch 32\n",
      "training top1 score: 0.9845132743362832\n",
      "training top5 score: 1.0\n",
      "starting epoch 33\n",
      "training top1 score: 0.9845132743362832\n",
      "training top5 score: 1.0\n",
      "starting epoch 34\n",
      "training top1 score: 0.9856194690265486\n",
      "training top5 score: 1.0\n",
      "starting epoch 35\n",
      "training top1 score: 0.9856194690265486\n",
      "training top5 score: 1.0\n",
      "starting epoch 36\n",
      "training top1 score: 0.9867256637168141\n",
      "training top5 score: 1.0\n",
      "starting epoch 37\n",
      "training top1 score: 0.9867256637168141\n",
      "training top5 score: 1.0\n",
      "starting epoch 38\n",
      "training top1 score: 0.9878318584070797\n",
      "training top5 score: 1.0\n",
      "starting epoch 39\n",
      "training top1 score: 0.9900442477876106\n",
      "training top5 score: 1.0\n",
      "starting epoch 40\n",
      "training top1 score: 0.9900442477876106\n",
      "training top5 score: 1.0\n",
      "starting epoch 41\n",
      "training top1 score: 0.9900442477876106\n",
      "training top5 score: 1.0\n",
      "starting epoch 42\n",
      "training top1 score: 0.9911504424778761\n",
      "training top5 score: 1.0\n",
      "starting epoch 43\n",
      "training top1 score: 0.9922566371681416\n",
      "training top5 score: 1.0\n",
      "starting epoch 44\n",
      "training top1 score: 0.9922566371681416\n",
      "training top5 score: 1.0\n",
      "starting epoch 45\n",
      "training top1 score: 0.9944690265486725\n",
      "training top5 score: 1.0\n",
      "starting epoch 46\n",
      "training top1 score: 0.9944690265486725\n",
      "training top5 score: 1.0\n",
      "starting epoch 47\n",
      "training top1 score: 0.9944690265486725\n",
      "training top5 score: 1.0\n",
      "starting epoch 48\n",
      "training top1 score: 0.9944690265486725\n",
      "training top5 score: 1.0\n",
      "starting epoch 49\n",
      "training top1 score: 0.995575221238938\n",
      "training top5 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    total_examples = 0\n",
    "    total_training_top1 = 0\n",
    "    total_training_top5 = 0\n",
    "    print('starting epoch %s' % epoch)\n",
    "    for batch, labels in val_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "        batch = batch.to(DEVICE)\n",
    "        logits = model(batch)\n",
    "        top1, top5 = accuracy(logits, labels)\n",
    "        total_training_top1 += top1\n",
    "        total_training_top5 += top5\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_examples += batch.shape[0]\n",
    "\n",
    "    training_top1_performance = total_training_top1 / total_examples\n",
    "    training_top5_performance = total_training_top5 / total_examples\n",
    "    print('training top1 score: %s' % training_top1_performance)\n",
    "    print('training top5 score: %s' % training_top5_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Evaluate when done <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.029411764705882353\n",
      "0.058823529411764705\n",
      "0.08823529411764706\n",
      "0.11764705882352941\n",
      "0.14705882352941177\n",
      "0.17647058823529413\n",
      "0.20588235294117646\n",
      "0.23529411764705882\n",
      "0.2647058823529412\n",
      "0.29411764705882354\n",
      "0.3235294117647059\n",
      "0.35294117647058826\n",
      "0.38235294117647056\n",
      "0.4117647058823529\n",
      "0.4411764705882353\n",
      "0.47058823529411764\n",
      "0.5\n",
      "0.5294117647058824\n",
      "0.5588235294117647\n",
      "0.5882352941176471\n",
      "0.6176470588235294\n",
      "0.6470588235294118\n",
      "0.6764705882352942\n",
      "0.7058823529411765\n",
      "0.7352941176470589\n",
      "0.7647058823529411\n",
      "0.7941176470588235\n",
      "0.8235294117647058\n",
      "0.8529411764705882\n",
      "0.8823529411764706\n",
      "0.9117647058823529\n",
      "0.9411764705882353\n",
      "0.9705882352941176\n",
      "top1 score 0.3274278987639788\n",
      "top5 score 0.5958799293702177\n"
     ]
    }
   ],
   "source": [
    "top1, top5 = evaluate()\n",
    "print('top1 score', top1)\n",
    "print('top5 score', top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 0.0001\n",
    "# top1 score 0.3578546635315194\n",
    "# top5 score 0.6243421789273318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 0.001 mistake\n",
    "# top1 score 0.36076587168290225\n",
    "# top5 score 0.6241182398387639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after refactoring with 0.001 on FULL objectnet\n",
    "# top1 score 0.20835247442054708\n",
    "# top5 score 0.4139277510962623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after refactoring with 0.0001\n",
    "# top1 score 0.3274278987639788\n",
    "# top5 score 0.5958799293702177"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
