{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import pickle\n",
    "from torch.optim import Adam\n",
    "import argparse\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/storage/jalverio/objectnet-oct-24-d123'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6f4be30413b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mon2onlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/storage/jalverio/objectnet-oct-24-d123'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mon2onlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0monlabel2name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mon2onlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/storage/jalverio/objectnet-oct-24-d123'"
     ]
    }
   ],
   "source": [
    "on2onlabel = dict()\n",
    "for idx, name in enumerate(os.listdir('/storage/jalverio/objectnet-oct-24-d123')):\n",
    "    on2onlabel[name] = idx\n",
    "onlabel2name = {v: k for k, v in on2onlabel.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/storage/jalverio/resnet/objectnet2torch.pkl', 'rb') as f:\n",
    "    objectnet2torch = pickle.load(f)\n",
    "torch2objectnet = dict()\n",
    "for objectnet_name, label_list in objectnet2torch.items():\n",
    "    for label in label_list:\n",
    "        torch2objectnet[label] = objectnet_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/storage/jalverio/resnet/dirname_to_objectnet_name.json') as f:\n",
    "    dirname_to_classname = json.load(f)\n",
    "\n",
    "with open('/storage/jalverio/resnet/objectnet_subset_to_objectnet_id') as f:\n",
    "    oncompressed2onlabel = eval(f.read())\n",
    "    onlabel2oncompressed = {v:k for k,v in oncompressed2onlabel.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transformations = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objectnet(Dataset):\n",
    "    def __init__(self, root, transform, objectnet2torch, num_examples, test, overlap, test_images=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        classes_in_dataset = set()\n",
    "        for dirname in os.listdir(root):\n",
    "            if overlap:\n",
    "                class_name = dirname_to_classname[dirname]\n",
    "                if class_name not in objectnet2torch:\n",
    "                    continue\n",
    "            classes_in_dataset.add(dirname)\n",
    "            label = on2onlabel[dirname]\n",
    "            import pdb; pdb.set_trace()\n",
    "            label = onlabel2oncompressed[label]\n",
    "            images = os.listdir(os.path.join(root, dirname))\n",
    "            for image_name in images:\n",
    "                path = os.path.join(root, dirname, image_name)\n",
    "                self.images.append((path, label))\n",
    "\n",
    "        if num_examples == 64:\n",
    "            self.remove_small_classes()\n",
    "\n",
    "        print('Created objectnet dataset with %s classes' % len(classes_in_dataset))\n",
    "        self.n_per_class(num_examples, test)\n",
    "\n",
    "        self.classes_in_dataset = classes_in_dataset\n",
    "\n",
    "    def remove_small_classes(self):\n",
    "        counter_dict = dict()\n",
    "        for _, label in self.images:\n",
    "            if label not in counter_dict:\n",
    "                counter_dict[label] = 1\n",
    "            else:\n",
    "                counter_dict[label] += 1\n",
    "        to_remove = []\n",
    "        for label, frequency in counter_dict.items():\n",
    "            if frequency < 64:\n",
    "                to_remove.append(label)\n",
    "        to_remove = set(to_remove)\n",
    "        new_images = []\n",
    "        for path, label in self.images:\n",
    "            if label not in to_remove:\n",
    "                new_images.append((path, label))\n",
    "        self.images = new_images\n",
    "\n",
    "    def n_per_class(self, num_examples, test):\n",
    "        valid_classes = set()\n",
    "        [valid_classes.add(label) for _, label in self.images]\n",
    "\n",
    "        quotas = dict()\n",
    "        for label in valid_classes:\n",
    "            quotas[label] = 0\n",
    "        test_images = []\n",
    "        remaining_images = []\n",
    "        for path, objectnet_label in self.images:\n",
    "            if not test:\n",
    "                if quotas[objectnet_label] < num_examples:\n",
    "                    quotas[objectnet_label] += 1\n",
    "                    remaining_images.append((path, objectnet_label))\n",
    "                else:\n",
    "                    test_images.append((path, objectnet_label))\n",
    "            else:\n",
    "                if quotas[objectnet_label] < num_examples * 2:\n",
    "                    if quotas[objectnet_label] >= num_examples:\n",
    "                        remaining_images.append((path, objectnet_label))\n",
    "                    quotas[objectnet_label] += 1\n",
    "                else:\n",
    "                    test_images.append((path, objectnet_label))\n",
    "        self.images = remaining_images\n",
    "        self.test_images = test_images\n",
    "        print('Removed some examples. %s classes and %s examples remaining.' % (len(valid_classes), len(self.images)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        full_path, labels = self.images[index]\n",
    "        image = Image.open(full_path)\n",
    "        image = image.convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        return image, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    total_top1, total_top5 = 0, 0\n",
    "    total_examples = 0\n",
    "    score_dict = dict()\n",
    "    for class_name in VALID_CLASSES:\n",
    "        score_dict[on2onlabel[class_name]] = np.zeros((2,))\n",
    "    for batch, labels in test_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "        batch = batch.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch)\n",
    "        # accuracy_results = accuracy_objectnet_nobatch(logits, labels)\n",
    "        top1, top5 = accuracy_objectnet(logits, labels)\n",
    "        # score_dict[labels.item()] += accuracy_results\n",
    "        total_top1 += top1\n",
    "        total_top5 += top5\n",
    "        total_examples += batch.shape[0]\n",
    "    top1_score = total_top1 / total_examples\n",
    "    top5_score = total_top5 / total_examples\n",
    "    SAVER.write_evaluation_record(top1_score, top5_score)\n",
    "    return top1_score, top5_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_objectnet(output, target):\n",
    "    with torch.no_grad():\n",
    "        _, pred = output.topk(5, 1, True, True)\n",
    "    top5_correct = 0\n",
    "    top1_correct = 0\n",
    "\n",
    "    for idx, prediction in enumerate(pred):\n",
    "        pred_set = set(prediction.cpu().numpy().tolist())\n",
    "        target_set = set([target[idx].cpu().numpy().tolist()])\n",
    "        if pred_set.intersection(target_set):\n",
    "            top5_correct += 1\n",
    "\n",
    "        if prediction[0].item() in target_set:\n",
    "            top1_correct += 1\n",
    "    return top1_correct, top5_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "WORKERS = 50\n",
    "BATCH_SIZE = 32\n",
    "NUM_EXAMPLES = 8\n",
    "OVERLAP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /Users/julianalverio/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n",
      "100%|██████████| 241530880/241530880 [00:18<00:00, 12775518.92it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet152(pretrained=True).eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(2048, 1000, bias=True)\n",
    "model = model.eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '/storage/jalverio/objectnet-oct-24-d123/'\n",
    "dataset = Objectnet(image_dir, transformations, objectnet2torch, N_EXAMPLES, test=False, overlap=OVERLAP)\n",
    "total_classes = len(dataset.classes_in_dataset)\n",
    "VALID_CLASSES = dataset.classes_in_dataset\n",
    "dataset_test = copy.deepcopy(dataset)\n",
    "dataset_test.images = dataset.test_images\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=256, shuffle=False,\n",
    "        num_workers=WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    total_examples = 0\n",
    "    total_training_top1 = 0\n",
    "    total_training_top5 = 0\n",
    "    print('starting epoch %s' % epoch)\n",
    "    for batch_counter, (batch, labels) in enumerate(val_loader):\n",
    "        labels = labels.to(DEVICE)\n",
    "        batch = batch.to(DEVICE)\n",
    "        logits = model(batch)\n",
    "        top1, top5 = accuracy_objectnet(logits, labels)\n",
    "        total_training_top1 += top1\n",
    "        total_training_top5 += top5\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_examples += batch.shape[0]\n",
    "\n",
    "    training_top1_performance = total_training_top1 / total_examples\n",
    "    training_top5_performance = total_training_top5 / total_examples\n",
    "    print('training top1 score: %s' % training_top1_performance)\n",
    "    print('training top5 score: %s' % training_top5_performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
