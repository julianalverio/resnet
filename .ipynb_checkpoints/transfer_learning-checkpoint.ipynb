{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> WARNING TODO: CHECK THAT TARGETS WILL ONLY EVER HAVE ONE LABEL </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import pickle\n",
    "from torch.optim import Adam\n",
    "import argparse\n",
    "import copy\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Get mappings </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "on2onlabel = dict()\n",
    "for idx, name in enumerate(os.listdir('/storage/jalverio/objectnet-oct-24-d123')):\n",
    "    on2onlabel[name] = idx\n",
    "onlabel2name = {v: k for k, v in on2onlabel.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/storage/jalverio/resnet/objectnet2torch.pkl', 'rb') as f:\n",
    "    objectnet2torch = pickle.load(f)\n",
    "torch2objectnet = dict()\n",
    "for objectnet_name, label_list in objectnet2torch.items():\n",
    "    for label in label_list:\n",
    "        torch2objectnet[label] = objectnet_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/storage/jalverio/resnet/dirname_to_objectnet_name.json') as f:\n",
    "    dirname_to_classname = json.load(f)\n",
    "\n",
    "with open('/storage/jalverio/resnet/objectnet_subset_to_objectnet_id') as f:\n",
    "    oncompressed2onlabel = eval(f.read())\n",
    "    onlabel2oncompressed = {v:k for k,v in oncompressed2onlabel.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Build Dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transformations = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 113, 313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objectnet(Dataset):\n",
    "    def __init__(self, root, transform, onlabel2oncompressed, num_examples, overlap, test_images=None):\n",
    "        self.transform = transform\n",
    "        if test_images is None:\n",
    "            self.classes_in_dataset = set()\n",
    "            images_dict = dict()\n",
    "            for dirname in os.listdir(root):\n",
    "                label = on2onlabel[dirname]\n",
    "                if overlap:\n",
    "                    if label not in onlabel2oncompressed:\n",
    "                        continue\n",
    "                    label = onlabel2oncompressed[label]\n",
    "                    class_name = dirname_to_classname[dirname]\n",
    "                images = os.listdir(os.path.join(root, dirname))\n",
    "                if len(images) < num_examples:\n",
    "                    continue\n",
    "                for image_name in images:\n",
    "                    path = os.path.join(root, dirname, image_name)\n",
    "                    if label not in images_dict:\n",
    "                        images_dict[label] = []\n",
    "                    images_dict[label].append(path)\n",
    "                self.classes_in_dataset.add(dirname)\n",
    "            self.images = []\n",
    "            self.test_images = []\n",
    "            for label in images_dict.keys():\n",
    "                try:\n",
    "                    idxs_to_choose_from = list(range(len(images_dict[label])))\n",
    "                    chosen_idxs = np.random.choice(idxs_to_choose_from, num_examples)\n",
    "                    class_training_idxs = set(chosen_idxs.tolist())\n",
    "                except:\n",
    "                    import pdb; pdb.set_trace()\n",
    "                class_training_images = [images_dict[label][idx] for idx in class_training_idxs]\n",
    "                test_training_idxs = [x for x in range(len(images_dict[label])) if x not in class_training_idxs]\n",
    "                class_test_images = [images_dict[label][idx] for idx in test_training_idxs]\n",
    "                [self.images.append((image, label)) for image in class_training_images]\n",
    "                [self.test_images.append((image, label)) for image in class_test_images]\n",
    "            print('Dataset has %s classes, %s training examples and %s test examples' % (len(self.classes_in_dataset), len(self.images), len(self.test_images)))\n",
    "        else:\n",
    "            self.images = test_images\n",
    "\n",
    "#     def n_per_class(self, num_examples, test):\n",
    "#         valid_classes = set()\n",
    "#         [valid_classes.add(label) for _, label in self.images]\n",
    "\n",
    "#         quotas = dict()\n",
    "#         for label in valid_classes:\n",
    "#             quotas[label] = 0\n",
    "#         test_images = []\n",
    "#         remaining_images = []\n",
    "#         for path, objectnet_label in self.images:\n",
    "#             if not test:\n",
    "#                 if quotas[objectnet_label] < num_examples:\n",
    "#                     quotas[objectnet_label] += 1\n",
    "#                     remaining_images.append((path, objectnet_label))\n",
    "#                 else:\n",
    "#                     test_images.append((path, objectnet_label))\n",
    "#             else:\n",
    "#                 if quotas[objectnet_label] < num_examples * 2:\n",
    "#                     if quotas[objectnet_label] >= num_examples:\n",
    "#                         remaining_images.append((path, objectnet_label))\n",
    "#                     quotas[objectnet_label] += 1\n",
    "#                 else:\n",
    "#                     test_images.append((path, objectnet_label))\n",
    "#         self.images = remaining_images\n",
    "#         self.test_images = test_images\n",
    "#         print('Removed some examples. %s classes and %s examples remaining.' % (len(valid_classes), len(self.images)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        full_path, labels = self.images[index]\n",
    "        image = Image.open(full_path)\n",
    "        image = image.convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        return image, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Helper functions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    total_top1, total_top5, total_examples = 0, 0, 0\n",
    "    for batch_counter, (batch, labels) in enumerate(test_loader):\n",
    "        print(batch_counter / len(test_loader))\n",
    "        labels = labels.to(DEVICE)\n",
    "        batch = batch.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch)\n",
    "        top1, top5 = accuracy(logits, labels)\n",
    "        total_top1 += top1\n",
    "        total_top5 += top5\n",
    "        total_examples += batch.shape[0]\n",
    "    top1_score = total_top1 / total_examples\n",
    "    top5_score = total_top5 / total_examples\n",
    "    return top1_score, top5_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, targets):\n",
    "    _, pred = logits.topk(5, 1, True, True)\n",
    "    targets = targets.unsqueeze(1)\n",
    "    targets_repeat = targets.repeat(1, 5)\n",
    "    assert pred.shape == targets_repeat.shape\n",
    "    correct = ((pred - targets_repeat) == 0).float()\n",
    "    top1_score = correct[:, 0].sum()\n",
    "    top5_score = correct.sum()\n",
    "    return top1_score.item(), top5_score.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> params and model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "WORKERS = 60\n",
    "BATCH_SIZE = 32\n",
    "NUM_EXAMPLES = 8\n",
    "OVERLAP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes_in_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-515c12b0de6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/storage/jalverio/objectnet-oct-24-d123/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monlabel2oncompressed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EXAMPLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOVERLAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monlabel2oncompressed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EXAMPLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOVERLAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_in_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-e9cf859f1995>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, onlabel2oncompressed, num_examples, overlap, test_images)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_training_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_test_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset has %s classes, %s training examples and %s test examples'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_in_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classes_in_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "image_dir = '/storage/jalverio/objectnet-oct-24-d123/'\n",
    "dataset = Objectnet(image_dir, transformations, onlabel2oncompressed, NUM_EXAMPLES, OVERLAP, test_images=None)\n",
    "import pdb; pdb.set_trace()\n",
    "dataset_test = Objectnet(image_dir, transformations, onlabel2oncompressed, NUM_EXAMPLES, OVERLAP, test_images=dataset.test_images)\n",
    "total_classes = len(dataset.classes_in_dataset)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=256, shuffle=False,\n",
    "        num_workers=WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet152(pretrained=True).eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(2048, total_classes, bias=True)\n",
    "model = model.eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Training <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "previous_accuracy = 0.\n",
    "top_score = 0.\n",
    "total_top1, total_top5, total_examples = 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    total_examples = 0\n",
    "    total_training_top1 = 0\n",
    "    total_training_top5 = 0\n",
    "    print('starting epoch %s' % epoch)\n",
    "    for batch, labels in val_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "        batch = batch.to(DEVICE)\n",
    "        logits = model(batch)\n",
    "        top1, top5 = accuracy(logits, labels)\n",
    "        total_training_top1 += top1\n",
    "        total_training_top5 += top5\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_examples += batch.shape[0]\n",
    "\n",
    "    training_top1_performance = total_training_top1 / total_examples\n",
    "    training_top5_performance = total_training_top5 / total_examples\n",
    "    print('training top1 score: %s' % training_top1_performance)\n",
    "    print('training top5 score: %s' % training_top5_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Evaluate when done <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1, top5 = evaluate()\n",
    "print('top1 score', top1)\n",
    "print('top5 score', top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 0.0001\n",
    "# top1 score 0.3578546635315194\n",
    "# top5 score 0.6243421789273318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 0.001 mistake\n",
    "# top1 score 0.36076587168290225\n",
    "# top5 score 0.6241182398387639"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
